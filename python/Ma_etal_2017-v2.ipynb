{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import mat73\n",
    "\n",
    "print(tf.__version__)\n",
    "# Test if tensorflow is using GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features(bands, std=0.0, featureMeans=None, featureSTDs=None):\n",
    "    if featureMeans is None:\n",
    "        featureMeans = np.mean(\n",
    "            bands,\n",
    "            axis=(0, 1))\n",
    "        featureSTDs = np.std(\n",
    "            bands,\n",
    "            axis=(0, 1))\n",
    "\n",
    "    if std > 0:\n",
    "        rng = np.random.default_rng()\n",
    "        gaussianNoise = std * rng.standard_normal(size=bands.shape)\n",
    "        return (bands - featureMeans) / featureSTDs + gaussianNoise, featureMeans, featureSTDs\n",
    "    else:\n",
    "        return (bands - featureMeans) / featureSTDs, featureMeans, featureSTDs\n",
    "\n",
    "\n",
    "def read_features_and_labels(directory, mult=1, add_labels=180):\n",
    "    featureVectors = mat73.loadmat('{}/featureVectors.mat'.format(directory))\n",
    "    featureVectors = featureVectors['featureVectors']\n",
    "    labels = mat73.loadmat('{}/labels.mat'.format(directory))\n",
    "    labels = mult*labels['labels']\n",
    "\n",
    "    # Keep only the indices corresponding to frontal horizontal plane DoAs\n",
    "    ########################################################################\n",
    "    front_indices = np.where((labels <= 90) & (labels >= -90))\n",
    "    labels = labels[front_indices]\n",
    "    featureVectors = np.squeeze(featureVectors[front_indices, :, :, :])\n",
    "    ########################################################################\n",
    "\n",
    "    numSegs = featureVectors.shape[0]\n",
    "    featureVectors = np.reshape(\n",
    "        np.swapaxes(featureVectors, 0, 2),\n",
    "        (32, numSegs*99, 34),\n",
    "        order='F'\n",
    "    )\n",
    "\n",
    "    labels = np.repeat(labels, 99)\n",
    "    # One-Hot Encode\n",
    "    categorical_labels = tf.keras.utils.to_categorical(\n",
    "        (labels+add_labels)/5, num_classes=72)\n",
    "    return featureVectors, categorical_labels\n",
    "\n",
    "\n",
    "def split_helper(vectors, t_frames, file_numbers, labels):\n",
    "    x = np.zeros((32, len(file_numbers)*t_frames, 34))\n",
    "    y = np.zeros((len(file_numbers)*t_frames, 72))\n",
    "    for i in range(len(file_numbers)):\n",
    "        x[:, i*t_frames:(i+1)*t_frames, :] = vectors[\n",
    "            :,\n",
    "            file_numbers[i]*t_frames:(file_numbers[i]+1)*t_frames,\n",
    "            :]\n",
    "        y[i*t_frames:(i+1)*t_frames, :] = labels[\n",
    "            file_numbers[i]*t_frames:(file_numbers[i]+1)*t_frames, :]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def train_val_split(bandsGaussNorm, categorical_labels):\n",
    "    t_frames = 99\n",
    "    filesNum = int(bandsGaussNorm.shape[1]/t_frames)\n",
    "    trainFileNumbers = set(range(filesNum))\n",
    "    valFileNumbers = set()\n",
    "\n",
    "    percentageSplit = 0.1\n",
    "    sampleSize = percentageSplit * len(trainFileNumbers)\n",
    "    answerSize = 0\n",
    "\n",
    "    # Randomly select 10% of the total files included, where files is a unique\n",
    "    # permutation of the outer product (Azimuths X Voice_Samples X SNRs)\n",
    "    random.seed(42)\n",
    "    while answerSize < sampleSize:\n",
    "        r = random.randint(0, filesNum-1)\n",
    "        if r not in valFileNumbers:\n",
    "            answerSize += 1\n",
    "            valFileNumbers.add(r)\n",
    "\n",
    "    trainFileNumbers -= valFileNumbers\n",
    "    trainFileNumbers = list(trainFileNumbers)\n",
    "    valFileNumbers = list(valFileNumbers)\n",
    "\n",
    "    x_train, y_train = split_helper(bandsGaussNorm,\n",
    "                                    t_frames,\n",
    "                                    trainFileNumbers,\n",
    "                                    categorical_labels)\n",
    "    x_val, y_val = split_helper(bandsGaussNorm,\n",
    "                                t_frames,\n",
    "                                valFileNumbers,\n",
    "                                categorical_labels)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n",
    "def read_train_wrapper(directory):\n",
    "    bands, categorical_labels = read_features_and_labels(directory)\n",
    "    bandsGaussNorm, fMeans, fStds = standardize_features(bands, std=0.4)\n",
    "    x_train, y_train, x_val, y_val = train_val_split(bandsGaussNorm, categorical_labels)\n",
    "    return x_train, y_train, x_val, y_val, fMeans, fStds\n",
    "\n",
    "\n",
    "def read_test_wrapper(directory, fMeans, fStds, mult, add_labels):\n",
    "    bands, y_test = read_features_and_labels(directory, mult=mult, add_labels=add_labels)\n",
    "    x_test, _, _ = standardize_features(bands, featureMeans=fMeans, featureSTDs=fStds)\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory of featureVectors.mat and labels.mat\n",
    "dir_in = ''\n",
    "x_train, y_train, x_val, y_val, fMeans, fStds = read_train_wrapper(dir_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics to use for testing later\n",
    "np.save('fMeans.npy', fMeans)\n",
    "np.save('fStd.npy', fStds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(34))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(72, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    # Reduce LR every epoch by 0.05 and keep steady after 20 epochs\n",
    "    if epoch >= 20:\n",
    "        return 0.05\n",
    "    else:\n",
    "        return -0.05*epoch + 1.00\n",
    "\n",
    "\n",
    "# Used for debuggin only\n",
    "class EpochBeginCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print('Epoch begins at: {:%H:%M:%S}'.format(\n",
    "            datetime.now()))\n",
    "        print(self.model.optimizer.learning_rate)\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session\n",
    "models = []\n",
    "for i in range(32):\n",
    "    models.append(createModel())\n",
    "    models[i].compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(\n",
    "            learning_rate=1.0, momentum=0.5, nesterov=False, name='SGD'),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "\n",
    "# ebc = EpochBeginCallback()\n",
    "LRScheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "callbacks = [LRScheduler,\n",
    "             tf.keras.callbacks.EarlyStopping(\n",
    "                 monitor='val_loss', min_delta=0, patience=20, verbose=1)]\n",
    "\n",
    "history = []\n",
    "for freqBand in range(32):\n",
    "    print('Training model {} begins at: {:%H:%M:%S}'.format(\n",
    "        freqBand+1, datetime.now()))\n",
    "    history.append(\n",
    "        models[freqBand].fit(\n",
    "            x=x_train[freqBand, :],\n",
    "            y=y_train,\n",
    "            validation_data=(x_val[freqBand, :], y_val),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[callbacks],\n",
    "            verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Models as h5 files for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expNumSave = 5\n",
    "for freqBand in range(32):\n",
    "    models[freqBand].save(\n",
    "        'Models_History/Ma_2017/Experiment_{}_Epochs_{}/freqBand_{}.h5'.format(\n",
    "            expNumSave, EPOCHS, freqBand+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "history = []\n",
    "for freqBand in range(32):\n",
    "    models.append(tf.keras.models.load_model(\n",
    "        r'G:\\My Drive\\AudioGroup\\PhD_Code\\binaural_compression_DoA_estimation\\jup_notebooks\\Models_History\\Ma_2017\\Experiment_4_Epochs_25/freqBand_{}.h5'.format(freqBand+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fMeans = np.load('fMeans.npy')\n",
    "fStds = np.load('fStd.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directory of featureVectors.mat and labels.mat for the test set\n",
    "test_dir = ''\n",
    "x_test, y_test = read_test_wrapper(test_dir, fMeans, fStds, -1, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate per frame accuracy\n",
    "acc = np.zeros((32, 1))\n",
    "for fBand in range(32):\n",
    "    print('Band: {}'.format(fBand))\n",
    "    _, acc[fBand] = models[fBand].evaluate(x_test[fBand, :, :], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate to the model that DoAs are only in the frontal horizontal plane\n",
    "p_front = np.zeros((72))\n",
    "p_front[18:55] = np.ones((1, 55-18))\n",
    "\n",
    "# For equiprobable DoA\n",
    "# p_front = np.ones((72))\n",
    "\n",
    "t_frames = 99\n",
    "correctCount = 0\n",
    "totalCount = int(x_test.shape[1] / t_frames)\n",
    "azPreds = np.zeros((totalCount, 1))\n",
    "azTrue = np.zeros((totalCount, 1))\n",
    "\n",
    "for fNum in range(totalCount):\n",
    "    fileToEstimate = x_test[:, fNum*t_frames:(fNum+1)*t_frames]\n",
    "    y_preds = np.zeros((32, t_frames, 72))\n",
    "    for freqBand in range(32):\n",
    "        y_preds[freqBand] = models[freqBand].predict(fileToEstimate[freqBand, :, :])\n",
    "\n",
    "    tmp = p_front * np.prod(y_preds, axis=0)  # Product accros all frequencies dims: [t_frames,72]\n",
    "    freqIntegral = tmp / np.sum(tmp, axis=1)[:, None]  # Integrate accross frequency, dims:[t_frames,72]\n",
    "    timeIntegral = np.sum(freqIntegral, axis=0) / t_frames\n",
    "\n",
    "    azPreds[fNum] = np.argmax(timeIntegral)\n",
    "    azTrue[fNum] = np.argmax(y_test[fNum*t_frames])\n",
    "\n",
    "    if np.argmax(timeIntegral) == np.argmax(y_test[fNum*t_frames]):\n",
    "        correctCount += 1\n",
    "\n",
    "print('Localization Accuracy: {:.2f}%'.format(100*correctCount / totalCount))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
